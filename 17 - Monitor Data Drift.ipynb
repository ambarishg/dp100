{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring Data Drift\n",
    "\n",
    "Over time, models can become less effective at predicting accurately due to changing trends in feature data. This phenomenon is known as *data drift*, and it's important to monitor your machine learning solution to detect it so you can retrain your models if necessary.\n",
    "\n",
    "In this lab, you'll configure data drift monitoring for datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "In addition to the latest version of the **azureml-sdk** and **azureml-widgets** packages, you'll need the **azureml-datadrift** package to run the code in this notebook. Run the cell below to verify that it is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azureml-datadrift\n",
      "  Downloading azureml_datadrift-1.34.0-py3-none-any.whl (99 kB)\n",
      "Requirement already satisfied: msrest>=0.5.1 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-datadrift) (0.6.21)\n",
      "Collecting azureml-dataset-runtime[fuse,pandas]~=1.34.0\n",
      "  Downloading azureml_dataset_runtime-1.34.0-py3-none-any.whl (3.5 kB)\n",
      "Collecting matplotlib<=3.2.1,>=3.0.2\n",
      "  Downloading matplotlib-3.2.1-cp38-cp38-win_amd64.whl (9.2 MB)\n",
      "Collecting azureml-pipeline-core~=1.34.0\n",
      "  Downloading azureml_pipeline_core-1.34.0-py3-none-any.whl (312 kB)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.2.1-py3-none-win_amd64.whl (1.0 MB)\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
      "Collecting azureml-core~=1.34.0\n",
      "  Downloading azureml_core-1.34.0-py3-none-any.whl (2.2 MB)\n",
      "Requirement already satisfied: numpy in c:\\applications\\anaconda\\lib\\site-packages (from azureml-datadrift) (1.18.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\applications\\anaconda\\lib\\site-packages (from azureml-datadrift) (0.24.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-datadrift) (1.5.2)\n",
      "Requirement already satisfied: jsonpickle in c:\\applications\\anaconda\\lib\\site-packages (from azureml-datadrift) (2.0.0)\n",
      "Collecting azureml-telemetry~=1.34.0\n",
      "  Downloading azureml_telemetry-1.34.0-py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: pandas in c:\\applications\\anaconda\\lib\\site-packages (from azureml-datadrift) (1.2.4)\n",
      "Requirement already satisfied: azure-mgmt-keyvault<10.0.0,>=0.40.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (9.0.0)\n",
      "Requirement already satisfied: SecretStorage<4.0.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (3.3.1)\n",
      "Requirement already satisfied: azure-graphrbac<1.0.0,>=0.40.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (0.61.1)\n",
      "Requirement already satisfied: azure-mgmt-storage<16.0.0,>=1.5.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (11.2.0)\n",
      "Requirement already satisfied: pyopenssl<21.0.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (19.1.0)\n",
      "Requirement already satisfied: ndg-httpsclient<=0.5.1 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (0.5.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.19.1 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (2.25.1)\n",
      "Requirement already satisfied: PyJWT<3.0.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (1.7.1)\n",
      "Requirement already satisfied: pytz in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (2021.1)\n",
      "Requirement already satisfied: cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<4.0.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (3.3.2)\n",
      "Requirement already satisfied: azure-mgmt-containerregistry>=2.0.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (8.0.0)\n",
      "Requirement already satisfied: adal<=1.2.7,>=1.2.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (1.2.7)\n",
      "Requirement already satisfied: backports.tempfile in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (1.0)\n",
      "Requirement already satisfied: contextlib2<22.0.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (0.6.0.post1)\n",
      "Requirement already satisfied: docker<6.0.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (4.4.4)\n",
      "Requirement already satisfied: azure-common<2.0.0,>=1.1.12 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (1.1.27)\n",
      "Requirement already satisfied: ruamel.yaml<0.17.5,>=0.15.35 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (0.17.4)\n",
      "Requirement already satisfied: jmespath<1.0.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (0.10.0)\n",
      "Requirement already satisfied: urllib3<=1.26.6,>=1.23 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (1.25.11)\n",
      "Requirement already satisfied: azure-mgmt-resource<15.0.0,>=1.2.1 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (13.0.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.3 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (2.8.1)\n",
      "Requirement already satisfied: pathspec<1.0.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (0.7.0)\n",
      "Requirement already satisfied: azure-mgmt-authorization<1.0.0,>=0.40.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (0.61.0)\n",
      "Requirement already satisfied: msrestazure<=0.6.4,>=0.4.33 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-core~=1.34.0->azureml-datadrift) (0.6.4)\n",
      "Requirement already satisfied: azure-mgmt-core<2.0.0,>=1.2.0 in c:\\applications\\anaconda\\lib\\site-packages (from azure-mgmt-containerregistry>=2.0.0->azureml-core~=1.34.0->azureml-datadrift) (1.2.2)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.9.0 in c:\\applications\\anaconda\\lib\\site-packages (from azure-mgmt-core<2.0.0,>=1.2.0->azure-mgmt-containerregistry>=2.0.0->azureml-core~=1.34.0->azureml-datadrift) (1.14.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\applications\\anaconda\\lib\\site-packages (from azure-core<2.0.0,>=1.9.0->azure-mgmt-core<2.0.0,>=1.2.0->azure-mgmt-containerregistry>=2.0.0->azureml-core~=1.34.0->azureml-datadrift) (1.15.0)\n",
      "Collecting azureml-dataprep<2.23.0a,>=2.22.0a\n",
      "  Downloading azureml_dataprep-2.22.2-py3-none-any.whl (39.4 MB)\n",
      "Requirement already satisfied: pyarrow<4.0.0,>=0.17.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-dataset-runtime[fuse,pandas]~=1.34.0->azureml-datadrift) (1.0.1)\n",
      "Collecting fusepy<4.0.0,>=3.0.1\n",
      "  Using cached fusepy-3.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: azure-identity<1.5.0,>=1.2.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-dataprep<2.23.0a,>=2.22.0a->azureml-dataset-runtime[fuse,pandas]~=1.34.0->azureml-datadrift) (1.4.1)\n",
      "Requirement already satisfied: azureml-dataprep-native<39.0.0,>=38.0.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-dataprep<2.23.0a,>=2.22.0a->azureml-dataset-runtime[fuse,pandas]~=1.34.0->azureml-datadrift) (38.0.0)\n",
      "Requirement already satisfied: dotnetcore2<3.0.0,>=2.1.14 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-dataprep<2.23.0a,>=2.22.0a->azureml-dataset-runtime[fuse,pandas]~=1.34.0->azureml-datadrift) (2.1.21)\n",
      "Requirement already satisfied: cloudpickle<2.0.0,>=1.1.0 in c:\\applications\\anaconda\\lib\\site-packages (from azureml-dataprep<2.23.0a,>=2.22.0a->azureml-dataset-runtime[fuse,pandas]~=1.34.0->azureml-datadrift) (1.6.0)\n",
      "Collecting azureml-dataprep-rslex~=1.20.0dev0\n",
      "  Downloading azureml_dataprep_rslex-1.20.2-cp38-cp38-win_amd64.whl (9.1 MB)\n",
      "Requirement already satisfied: msal-extensions~=0.2.2 in c:\\applications\\anaconda\\lib\\site-packages (from azure-identity<1.5.0,>=1.2.0->azureml-dataprep<2.23.0a,>=2.22.0a->azureml-dataset-runtime[fuse,pandas]~=1.34.0->azureml-datadrift) (0.2.2)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.3.0 in c:\\applications\\anaconda\\lib\\site-packages (from azure-identity<1.5.0,>=1.2.0->azureml-dataprep<2.23.0a,>=2.22.0a->azureml-dataset-runtime[fuse,pandas]~=1.34.0->azureml-datadrift) (1.13.0)\n",
      "Requirement already satisfied: applicationinsights in c:\\applications\\anaconda\\lib\\site-packages (from azureml-telemetry~=1.34.0->azureml-datadrift) (0.11.10)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\applications\\anaconda\\lib\\site-packages (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<4.0.0->azureml-core~=1.34.0->azureml-datadrift) (1.14.5)\n",
      "Requirement already satisfied: pycparser in c:\\applications\\anaconda\\lib\\site-packages (from cffi>=1.12->cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<4.0.0->azureml-core~=1.34.0->azureml-datadrift) (2.20)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in c:\\applications\\anaconda\\lib\\site-packages (from docker<6.0.0->azureml-core~=1.34.0->azureml-datadrift) (1.0.1)\n",
      "Requirement already satisfied: pywin32==227 in c:\\applications\\anaconda\\lib\\site-packages (from docker<6.0.0->azureml-core~=1.34.0->azureml-datadrift) (227)\n",
      "Requirement already satisfied: distro>=1.2.0 in c:\\applications\\anaconda\\lib\\site-packages (from dotnetcore2<3.0.0,>=2.1.14->azureml-dataprep<2.23.0a,>=2.22.0a->azureml-dataset-runtime[fuse,pandas]~=1.34.0->azureml-datadrift) (1.6.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\applications\\anaconda\\lib\\site-packages (from matplotlib<=3.2.1,>=3.0.2->azureml-datadrift) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\applications\\anaconda\\lib\\site-packages (from matplotlib<=3.2.1,>=3.0.2->azureml-datadrift) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\applications\\anaconda\\lib\\site-packages (from matplotlib<=3.2.1,>=3.0.2->azureml-datadrift) (1.3.1)\n",
      "Requirement already satisfied: portalocker~=1.6 in c:\\applications\\anaconda\\lib\\site-packages (from msal-extensions~=0.2.2->azure-identity<1.5.0,>=1.2.0->azureml-dataprep<2.23.0a,>=2.22.0a->azureml-dataset-runtime[fuse,pandas]~=1.34.0->azureml-datadrift) (1.7.1)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\applications\\anaconda\\lib\\site-packages (from msrest>=0.5.1->azureml-datadrift) (0.6.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\applications\\anaconda\\lib\\site-packages (from msrest>=0.5.1->azureml-datadrift) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\applications\\anaconda\\lib\\site-packages (from msrest>=0.5.1->azureml-datadrift) (2020.12.5)\n",
      "Requirement already satisfied: pyasn1>=0.1.1 in c:\\applications\\anaconda\\lib\\site-packages (from ndg-httpsclient<=0.5.1->azureml-core~=1.34.0->azureml-datadrift) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\applications\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.19.1->azureml-core~=1.34.0->azureml-datadrift) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\applications\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.19.1->azureml-core~=1.34.0->azureml-datadrift) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\applications\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.1->azureml-datadrift) (3.1.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.1.2 in c:\\applications\\anaconda\\lib\\site-packages (from ruamel.yaml<0.17.5,>=0.15.35->azureml-core~=1.34.0->azureml-datadrift) (0.2.2)\n",
      "Requirement already satisfied: jeepney>=0.6 in c:\\applications\\anaconda\\lib\\site-packages (from SecretStorage<4.0.0->azureml-core~=1.34.0->azureml-datadrift) (0.6.0)\n",
      "Requirement already satisfied: backports.weakref in c:\\applications\\anaconda\\lib\\site-packages (from backports.tempfile->azureml-core~=1.34.0->azureml-datadrift) (1.0.post1)\n",
      "Requirement already satisfied: wheel in c:\\applications\\anaconda\\lib\\site-packages (from lightgbm->azureml-datadrift) (0.36.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\applications\\anaconda\\lib\\site-packages (from scikit-learn->azureml-datadrift) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\applications\\anaconda\\lib\\site-packages (from scikit-learn->azureml-datadrift) (0.14.1)\n",
      "Collecting py4j==0.10.9\n",
      "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=42ccf89c3c47428ff570978101c400da9988dad614d35ae3e00d1a92193ec1da\n",
      "  Stored in directory: c:\\users\\ambar\\appdata\\local\\pip\\cache\\wheels\\df\\88\\9e\\58ef1f74892fef590330ca0830b5b6d995ba29b44f977b3926\n",
      "Successfully built pyspark\n",
      "Installing collected packages: azureml-dataprep-rslex, azureml-dataprep, py4j, fusepy, azureml-dataset-runtime, azureml-core, pyspark, matplotlib, lightgbm, azureml-telemetry, azureml-pipeline-core, azureml-datadrift\n",
      "  Attempting uninstall: azureml-dataprep-rslex\n",
      "    Found existing installation: azureml-dataprep-rslex 1.18.2\n",
      "    Uninstalling azureml-dataprep-rslex-1.18.2:\n",
      "      Successfully uninstalled azureml-dataprep-rslex-1.18.2\n",
      "  Attempting uninstall: azureml-dataprep\n",
      "    Found existing installation: azureml-dataprep 2.20.1\n",
      "    Uninstalling azureml-dataprep-2.20.1:\n",
      "      Successfully uninstalled azureml-dataprep-2.20.1\n",
      "  Attempting uninstall: azureml-dataset-runtime\n",
      "    Found existing installation: azureml-dataset-runtime 1.33.0\n",
      "    Uninstalling azureml-dataset-runtime-1.33.0:\n",
      "      Successfully uninstalled azureml-dataset-runtime-1.33.0\n",
      "  Attempting uninstall: azureml-core\n",
      "    Found existing installation: azureml-core 1.33.0\n",
      "    Uninstalling azureml-core-1.33.0:\n",
      "      Successfully uninstalled azureml-core-1.33.0\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.3.4\n",
      "    Uninstalling matplotlib-3.3.4:\n",
      "      Successfully uninstalled matplotlib-3.3.4\n",
      "  Attempting uninstall: azureml-telemetry\n",
      "    Found existing installation: azureml-telemetry 1.33.0\n",
      "    Uninstalling azureml-telemetry-1.33.0:\n",
      "      Successfully uninstalled azureml-telemetry-1.33.0\n",
      "  Attempting uninstall: azureml-pipeline-core\n",
      "    Found existing installation: azureml-pipeline-core 1.33.0\n",
      "    Uninstalling azureml-pipeline-core-1.33.0:\n",
      "      Successfully uninstalled azureml-pipeline-core-1.33.0\n",
      "Successfully installed azureml-core-1.34.0 azureml-datadrift-1.34.0 azureml-dataprep-2.22.2 azureml-dataprep-rslex-1.20.2 azureml-dataset-runtime-1.34.0 azureml-pipeline-core-1.34.0 azureml-telemetry-1.34.0 fusepy-3.0.1 lightgbm-3.2.1 matplotlib-3.2.1 py4j-0.10.9 pyspark-3.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "azureml-widgets 1.33.0 requires azureml-core~=1.33.0, but you have azureml-core 1.34.0 which is incompatible.\n",
      "azureml-widgets 1.33.0 requires azureml-telemetry~=1.33.0, but you have azureml-telemetry 1.34.0 which is incompatible.\n",
      "azureml-train-core 1.33.0 requires azureml-core~=1.33.0, but you have azureml-core 1.34.0 which is incompatible.\n",
      "azureml-train-core 1.33.0 requires azureml-telemetry~=1.33.0, but you have azureml-telemetry 1.34.0 which is incompatible.\n",
      "azureml-train-automl-client 1.33.0 requires azureml-core~=1.33.0, but you have azureml-core 1.34.0 which is incompatible.\n",
      "azureml-train-automl-client 1.33.0 requires azureml-dataset-runtime~=1.33.0, but you have azureml-dataset-runtime 1.34.0 which is incompatible.\n",
      "azureml-train-automl-client 1.33.0 requires azureml-telemetry~=1.33.0, but you have azureml-telemetry 1.34.0 which is incompatible.\n",
      "azureml-pipeline 1.33.0 requires azureml-pipeline-core~=1.33.0, but you have azureml-pipeline-core 1.34.0 which is incompatible.\n",
      "azureml-pipeline-steps 1.33.0 requires azureml-pipeline-core~=1.33.0, but you have azureml-pipeline-core 1.34.0 which is incompatible.\n",
      "azureml-mlflow 1.33.0 requires azureml-core~=1.33.0, but you have azureml-core 1.34.0 which is incompatible.\n",
      "azureml-interpret 1.33.0 requires azureml-core~=1.33.0, but you have azureml-core 1.34.0 which is incompatible.\n",
      "azureml-automl-core 1.33.0 requires azureml-dataset-runtime~=1.33.0, but you have azureml-dataset-runtime 1.34.0 which is incompatible.\n",
      "azureml-automl-core 1.33.0 requires azureml-telemetry~=1.33.0, but you have azureml-telemetry 1.34.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install azureml-datadrift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to your workspace\n",
    "\n",
    "With the required SDK packages installed, now you're ready to connect to your workspace.\n",
    "\n",
    "> **Note**: If you haven't already established an authenticated session with your Azure subscription, you'll be prompted to authenticate by clicking a link, entering an authentication code, and signing into Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (azureml-telemetry 1.34.0 (c:\\applications\\anaconda\\lib\\site-packages), Requirement.parse('azureml-telemetry~=1.33.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (azureml-telemetry 1.34.0 (c:\\applications\\anaconda\\lib\\site-packages), Requirement.parse('azureml-telemetry~=1.33.0')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to work with wsag\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to work with', ws.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a *baseline* dataset\n",
    "\n",
    "To monitor a dataset for data drift, you must register a *baseline* dataset (usually the dataset used to train your model) to use as a point of comparison with data collected in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 2 files\n",
      "Uploading data/diabetes2.csv\n",
      "Uploaded data/diabetes2.csv, 1 files out of an estimated total of 2\n",
      "Uploading data/diabetes.csv\n",
      "Uploaded data/diabetes.csv, 2 files out of an estimated total of 2\n",
      "Uploaded 2 files\n",
      "Registering baseline dataset...\n",
      "Baseline dataset registered!\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Datastore, Dataset\n",
    "\n",
    "\n",
    "# Upload the baseline data\n",
    "default_ds = ws.get_default_datastore()\n",
    "default_ds.upload_files(files=['data/diabetes.csv', 'data/diabetes2.csv'],\n",
    "                       target_path='diabetes-baseline',\n",
    "                       overwrite=True, \n",
    "                       show_progress=True)\n",
    "\n",
    "# Create and register the baseline dataset\n",
    "print('Registering baseline dataset...')\n",
    "baseline_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-baseline/*.csv'))\n",
    "baseline_data_set = baseline_data_set.register(workspace=ws, \n",
    "                           name='diabetes baseline',\n",
    "                           description='diabetes baseline data',\n",
    "                           tags = {'format':'CSV'},\n",
    "                           create_new_version=True)\n",
    "\n",
    "print('Baseline dataset registered!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a *target* dataset\n",
    "\n",
    "Over time, you can collect new data with the same features as your baseline training data. To compare this new data to the baseline data, you must define a target dataset that includes the features you want to analyze for data drift as well as a timestamp field that indicates the point in time when the new data was current -this enables you to measure data drift over temporal intervals. The timestamp can either be a field in the dataset itself, or derived from the folder and filename pattern used to store the data. For example, you might store new data in a folder hierarchy that consists of a folder for the year, containing a folder for the month, which in turn contains a folder for the day; or you might just encode the year, month, and day in the file name like this: *data_2020-01-29.csv*; which is the approach taken in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating simulated data...\n",
      "Uploading an estimated of 6 files\n",
      "Uploading data/diabetes_2021-08-15.csv\n",
      "Uploaded data/diabetes_2021-08-15.csv, 1 files out of an estimated total of 6\n",
      "Uploading data/diabetes_2021-08-22.csv\n",
      "Uploaded data/diabetes_2021-08-22.csv, 2 files out of an estimated total of 6\n",
      "Uploading data/diabetes_2021-08-29.csv\n",
      "Uploaded data/diabetes_2021-08-29.csv, 3 files out of an estimated total of 6\n",
      "Uploading data/diabetes_2021-09-12.csv\n",
      "Uploaded data/diabetes_2021-09-12.csv, 4 files out of an estimated total of 6\n",
      "Uploading data/diabetes_2021-09-19.csv\n",
      "Uploaded data/diabetes_2021-09-19.csv, 5 files out of an estimated total of 6\n",
      "Uploading data/diabetes_2021-09-05.csv\n",
      "Uploaded data/diabetes_2021-09-05.csv, 6 files out of an estimated total of 6\n",
      "Uploaded 6 files\n",
      "Registering target dataset...\n",
      "Target dataset registered!\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "print('Generating simulated data...')\n",
    "\n",
    "# Load the smaller of the two data files\n",
    "data = pd.read_csv('data/diabetes2.csv')\n",
    "\n",
    "# We'll generate data for the past 6 weeks\n",
    "weeknos = reversed(range(6))\n",
    "\n",
    "file_paths = []\n",
    "for weekno in weeknos:\n",
    "    \n",
    "    # Get the date X weeks ago\n",
    "    data_date = dt.date.today() - dt.timedelta(weeks=weekno)\n",
    "    \n",
    "    # Modify data to ceate some drift\n",
    "    data['Pregnancies'] = data['Pregnancies'] + 1\n",
    "    data['Age'] = round(data['Age'] * 1.2).astype(int)\n",
    "    data['BMI'] = data['BMI'] * 1.1\n",
    "    \n",
    "    # Save the file with the date encoded in the filename\n",
    "    file_path = 'data/diabetes_{}.csv'.format(data_date.strftime(\"%Y-%m-%d\"))\n",
    "    data.to_csv(file_path)\n",
    "    file_paths.append(file_path)\n",
    "\n",
    "# Upload the files\n",
    "path_on_datastore = 'diabetes-target'\n",
    "default_ds.upload_files(files=file_paths,\n",
    "                       target_path=path_on_datastore,\n",
    "                       overwrite=True,\n",
    "                       show_progress=True)\n",
    "\n",
    "# Use the folder partition format to define a dataset with a 'date' timestamp column\n",
    "partition_format = path_on_datastore + '/diabetes_{date:yyyy-MM-dd}.csv'\n",
    "target_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, path_on_datastore + '/*.csv'),\n",
    "                                                       partition_format=partition_format)\n",
    "\n",
    "# Register the target dataset\n",
    "print('Registering target dataset...')\n",
    "target_data_set = target_data_set.with_timestamp_columns('date').register(workspace=ws,\n",
    "                                                                          name='diabetes target',\n",
    "                                                                          description='diabetes target data',\n",
    "                                                                          tags = {'format':'CSV'},\n",
    "                                                                          create_new_version=True)\n",
    "\n",
    "print('Target dataset registered!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a data drift monitor\n",
    "\n",
    "Now you're ready to create a data drift monitor for the diabetes data. The data drift monitor will run periodicaly or on-demand to compare the baseline dataset with the target dataset, to which new data will be added over time.\n",
    "\n",
    "### Create a compute target\n",
    "\n",
    "To run the data drift monitor, you'll need a compute target. Run the following cell to specify a compute cluster (if it doesn't exist, it will be created).\n",
    "\n",
    "> **Important**: Change *your-compute-cluster* to the name of your compute cluster in the code below before running it! Cluster names must be globally unique names between 2 to 16 characters in length. Valid characters are letters, digits, and the - character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InProgress...\n",
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"agcluster\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        training_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Compute instances and clusters are based on standard Azure virtual machine images. For this exercise, the *Standard_DS11_v2* image is recommended to achieve the optimal balance of cost and performance. If your subscription has a quota that does not include this image, choose an alternative image; but bear in mind that a larger image may incur higher cost and a smaller image may not be sufficient to complete the tasks. Alternatively, ask your Azure administrator to extend your quota.\n",
    "\n",
    "### Define the data drift monitor\n",
    "\n",
    "Now you're ready to use a **DataDriftDetector** class to define the data drift monitor for your data. You can specify the features you want to monitor for data drift, the name of the compute target to be used to run the monitoring process, the frequency at which the data should be compared, the data drift threshold above which an alert should be triggered, and the latency (in hours) to allow for data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_logger': <_TelemetryLoggerContextAdapter azureml.datadrift._logging._telemetry_logger.azureml.datadrift.datadriftdetector (DEBUG)>, '_workspace': Workspace.create(name='wsag', subscription_id='6ea869be-bab3-4204-94c3-1fc677f7d2de', resource_group='rgag'), '_frequency': 'Week', '_schedule_start': None, '_schedule_id': None, '_interval': 1, '_state': 'Disabled', '_alert_config': None, '_type': 'DatasetBased', '_id': '24361da5-7f77-4f02-8037-9398b22f37a1', '_compute_target_name': 'agcluster', '_drift_threshold': 0.3, '_baseline_dataset_id': '9602b31a-d59d-4bfe-a835-2432fe604406', '_target_dataset_id': 'a513d7d0-1ff3-4ea7-8bf0-014fb763b8d8', '_feature_list': ['Pregnancies', 'Age', 'BMI'], '_latency': 24, '_name': 'mslearn-diabates-drift', '_latest_run_time': None, '_client': <azureml.datadrift._restclient.datadrift_client.DataDriftClient object at 0x0000022158094D60>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.datadrift import DataDriftDetector\n",
    "\n",
    "# set up feature list\n",
    "features = ['Pregnancies', 'Age', 'BMI']\n",
    "\n",
    "# set up data drift detector\n",
    "monitor = DataDriftDetector.create_from_datasets(ws, 'mslearn-diabates-drift', baseline_data_set, target_data_set,\n",
    "                                                      compute_target=cluster_name, \n",
    "                                                      frequency='Week', \n",
    "                                                      feature_list=features, \n",
    "                                                      drift_threshold=.3, \n",
    "                                                      latency=24)\n",
    "monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backfill the data drift monitor\n",
    "\n",
    "You have a baseline dataset and a target dataset that includes simulated weekly data collection for six weeks. You can use this to backfill the monitor so that it can analyze data drift between the original baseline and the target data.\n",
    "\n",
    "> **Note** This may take some time to run, as the compute target must be started to run the backfill analysis. The widget may not always update to show the status, so click the link to observe the experiment status in Azure Machine Learning studio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd0bdbbf3a244b2bd1cc6c183cdf99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "\"UserErrorException:\\n\\tMessage: \\nOperation returned an invalid status code 'Forbidden'. The possible reason could be:\\n1. You are not authorized to access this resource, or directory listing denied.\\n2. you may not login your azure service, or use other subscription, you can check your\\ndefault account by running azure cli commend:\\n'az account list -o table'.\\n3. You have multiple objects/login session opened, please close all session and try again.\\n                \\n\\tInnerException None\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"\\\\nOperation returned an invalid status code 'Forbidden'. The possible reason could be:\\\\n1. You are not authorized to access this resource, or directory listing denied.\\\\n2. you may not login your azure service, or use other subscription, you can check your\\\\ndefault account by running azure cli commend:\\\\n'az account list -o table'.\\\\n3. You have multiple objects/login session opened, please close all session and try again.\\\\n                \\\"\\n    }\\n}\""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'mslearn-diabates-drift-Monitor-Runs_1632059782411',\n",
       " 'target': 'agcluster',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2021-09-19T14:10:31.065938Z',\n",
       " 'endTimeUtc': '2021-09-19T14:17:59.53249Z',\n",
       " 'services': {},\n",
       " 'properties': {'_azureml.ComputeTargetType': 'amlcompute',\n",
       "  'ContentSnapshotId': '8ef9dca9-4d31-47c1-b678-496e7c9715e9',\n",
       "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
       "  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n",
       " 'inputDatasets': [{'dataset': {'id': '9602b31a-d59d-4bfe-a835-2432fe604406'}, 'consumptionDetails': {'type': 'Reference'}}, {'dataset': {'id': 'a513d7d0-1ff3-4ea7-8bf0-014fb763b8d8'}, 'consumptionDetails': {'type': 'Reference'}}],\n",
       " 'outputDatasets': [],\n",
       " 'runDefinition': {'script': '_generate_script_datasets.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--baseline_dataset_id',\n",
       "   '9602b31a-d59d-4bfe-a835-2432fe604406',\n",
       "   '--target_dataset_id',\n",
       "   'a513d7d0-1ff3-4ea7-8bf0-014fb763b8d8',\n",
       "   '--workspace_name',\n",
       "   'wsag',\n",
       "   '--workspace_location',\n",
       "   'eastasia',\n",
       "   '--instrumentation_key',\n",
       "   '4c1c3893-d3d0-450b-b4ca-96d68bac82f2',\n",
       "   '--ai_endpoint',\n",
       "   'https://dc.applicationinsights.azure.com/v2/track',\n",
       "   '--subscription_id',\n",
       "   '6ea869be-bab3-4204-94c3-1fc677f7d2de',\n",
       "   '--enable_metric_logger',\n",
       "   'true',\n",
       "   '--run_type',\n",
       "   'BackFill',\n",
       "   '--drift_threshold',\n",
       "   '0',\n",
       "   '--datadrift_id',\n",
       "   '24361da5-7f77-4f02-8037-9398b22f37a1',\n",
       "   '--datadrift_run_id',\n",
       "   '46c5d53f-6a1f-40e0-8a55-0aa2d9de7d7f',\n",
       "   '--datadrift_name',\n",
       "   'mslearn-diabates-drift',\n",
       "   '--frequency',\n",
       "   'Week',\n",
       "   '--datadrift_configuration_type',\n",
       "   'DatasetBased',\n",
       "   '--start_date',\n",
       "   '2021-08-08',\n",
       "   '--end_date',\n",
       "   '2021-09-26',\n",
       "   '--features_whitelist',\n",
       "   'Pregnancies',\n",
       "   'Age',\n",
       "   'BMI'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'agcluster',\n",
       "  'dataReferences': {},\n",
       "  'data': {},\n",
       "  'outputData': {},\n",
       "  'datacaches': [],\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'instanceTypes': [],\n",
       "  'priority': None,\n",
       "  'credentialPassthrough': False,\n",
       "  'identity': None,\n",
       "  'environment': {'name': 'Experiment mslearn-diabates-drift-Monitor-Runs Environment',\n",
       "   'version': 'Autosave_2021-09-19T13:56:23Z_e44c4670',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'dependencies': ['python=3.6.2',\n",
       "      'scikit-learn',\n",
       "      'scipy>=1.0.0',\n",
       "      'numpy',\n",
       "      'lightgbm<=3.1.0',\n",
       "      'pandas',\n",
       "      'pyarrow>=0.11.0',\n",
       "      'jsonpickle',\n",
       "      'psutil',\n",
       "      {'pip': ['azureml-defaults==1.34.0', 'azureml-datadrift==1.34.0']}],\n",
       "     'name': 'azureml_363e5ba51b4163463f04cfa8830bcf34'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None,\n",
       "     'username': None,\n",
       "     'password': None}},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': None,\n",
       "   'enableMLflowTracking': False},\n",
       "  'spark': {'configuration': {}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1,\n",
       "   'location': None},\n",
       "  'aiSuperComputer': {'instanceType': 'D2',\n",
       "   'imageVersion': 'pytorch-1.7.0',\n",
       "   'location': None,\n",
       "   'aiSuperComputerStorageData': None,\n",
       "   'interactive': False,\n",
       "   'scalePolicy': None,\n",
       "   'virtualClusterArmId': None,\n",
       "   'tensorboardLogDirectory': None,\n",
       "   'sshPublicKey': None,\n",
       "   'enableAzmlInt': True,\n",
       "   'priority': 'Medium',\n",
       "   'slaTier': 'Standard',\n",
       "   'userAlias': None},\n",
       "  'kubernetesCompute': {'instanceType': None},\n",
       "  'tensorflow': {'workerCount': 0, 'parameterServerCount': 0},\n",
       "  'mpi': {'processCountPerNode': 0},\n",
       "  'pyTorch': {'communicationBackend': None, 'processCount': None},\n",
       "  'hdi': {'yarnDeployMode': 'None'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'commandReturnCodeConfig': {'returnCode': 'Zero',\n",
       "   'successfulReturnCodes': []},\n",
       "  'environmentVariables': {},\n",
       "  'applicationEndpoints': {},\n",
       "  'parameters': [],\n",
       "  'dataBricks': {'workers': 0,\n",
       "   'minimumWorkerCount': 0,\n",
       "   'maxMumWorkerCount': 0,\n",
       "   'sparkVersion': '4.0.x-scala2.11',\n",
       "   'nodeTypeId': 'Standard_D3_v2',\n",
       "   'sparkConf': {},\n",
       "   'sparkEnvVars': {},\n",
       "   'instancePoolId': None,\n",
       "   'timeoutSeconds': 0,\n",
       "   'jarLibraries': [],\n",
       "   'eggLibraries': [],\n",
       "   'whlLibraries': [],\n",
       "   'pypiLibraries': [],\n",
       "   'rCranLibraries': [],\n",
       "   'mavenLibraries': []}},\n",
       " 'logFiles': {'azureml-logs/20_image_build_log.txt': 'https://wsag2556001526.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1632059782411/azureml-logs/20_image_build_log.txt?sv=2019-07-07&sr=b&sig=JG9ODfFui0LXDwQQhsxKf3ac1Vl88vQ0YRfJms3cQLM%3D&st=2021-09-19T14%3A08%3A17Z&se=2021-09-19T22%3A18%3A17Z&sp=r',\n",
       "  'azureml-logs/55_azureml-execution-tvmps_5a04006a29586b4cdd564b7610303decb3d098a8c2353228075d0e92a31ed0de_d.txt': 'https://wsag2556001526.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1632059782411/azureml-logs/55_azureml-execution-tvmps_5a04006a29586b4cdd564b7610303decb3d098a8c2353228075d0e92a31ed0de_d.txt?sv=2019-07-07&sr=b&sig=skSzvvLRr6noBHLUQW59EzC%2FEFwKf1TBoqTS64RA%2Bbg%3D&st=2021-09-19T14%3A08%3A17Z&se=2021-09-19T22%3A18%3A17Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_5a04006a29586b4cdd564b7610303decb3d098a8c2353228075d0e92a31ed0de_d.txt': 'https://wsag2556001526.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1632059782411/azureml-logs/65_job_prep-tvmps_5a04006a29586b4cdd564b7610303decb3d098a8c2353228075d0e92a31ed0de_d.txt?sv=2019-07-07&sr=b&sig=eZLMIsEi5IivxKBlDcxuu%2BcPpMYQbbbaLYw6JIjlK1g%3D&st=2021-09-19T14%3A08%3A17Z&se=2021-09-19T22%3A18%3A17Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://wsag2556001526.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1632059782411/azureml-logs/70_driver_log.txt?sv=2019-07-07&sr=b&sig=0cFVB9OyiN5BTTtuoZtWQXXvrw6p610dRgM%2B1Nk6Q9E%3D&st=2021-09-19T14%3A08%3A17Z&se=2021-09-19T22%3A18%3A17Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_5a04006a29586b4cdd564b7610303decb3d098a8c2353228075d0e92a31ed0de_d.txt': 'https://wsag2556001526.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1632059782411/azureml-logs/75_job_post-tvmps_5a04006a29586b4cdd564b7610303decb3d098a8c2353228075d0e92a31ed0de_d.txt?sv=2019-07-07&sr=b&sig=AayO%2B8ywqlj3jdPNwUPdisgL%2Ff98cVHL1KVh11jH0hc%3D&st=2021-09-19T14%3A08%3A17Z&se=2021-09-19T22%3A18%3A17Z&sp=r',\n",
       "  'azureml-logs/process_info.json': 'https://wsag2556001526.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1632059782411/azureml-logs/process_info.json?sv=2019-07-07&sr=b&sig=lPU3s%2B067VahRZyYAUIBtS2X%2FxG0R%2FD%2B0N4FUgOLpPM%3D&st=2021-09-19T14%3A08%3A17Z&se=2021-09-19T22%3A18%3A17Z&sp=r',\n",
       "  'azureml-logs/process_status.json': 'https://wsag2556001526.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1632059782411/azureml-logs/process_status.json?sv=2019-07-07&sr=b&sig=PBlSVZIaMZIBsr8%2B5WIqHyMk3tt8833tHENBJvziqLo%3D&st=2021-09-19T14%3A08%3A17Z&se=2021-09-19T22%3A18%3A17Z&sp=r'},\n",
       " 'submittedBy': 'Ambarish Ganguly'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "backfill = monitor.backfill(dt.datetime.now() - dt.timedelta(weeks=6), dt.datetime.now())\n",
    "\n",
    "RunDetails(backfill).show()\n",
    "backfill.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze data drift\n",
    "\n",
    "You can use the following code to examine data drift for the points in time collected in the backfill run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_date 2021-08-01\n",
      "end_date 2021-09-19\n",
      "frequency Week\n",
      "Datadrift percentage {'days_from_start': [7, 14, 21, 28, 35, 42], 'drift_percentage': [74.19152901127207, 87.23985219136877, 91.74192122865539, 94.96492628559955, 97.58354951107833, 99.23199438682525]}\n"
     ]
    }
   ],
   "source": [
    "drift_metrics = backfill.get_metrics()\n",
    "for metric in drift_metrics:\n",
    "    print(metric, drift_metrics[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also visualize the data drift metrics in [Azure Machine Learning studio](https://ml.azure.com) by following these steps:\n",
    "\n",
    "1. On the **Datasets** page, view the **Dataset monitors** tab.\n",
    "2. Click the data drift monitor you want to view.\n",
    "3. Select the date range over which you want to view data drift metrics (if the column chart does not show multiple weeks of data, wait a minute or so and click **Refresh**).\n",
    "4. Examine the charts in the **Drift overview** section at the top, which show overall drift magnitude and the drift contribution per feature.\n",
    "5. Explore the charts in the **Feature detail** section at the bottom, which enable you to see various measures of drift for individual features.\n",
    "\n",
    "> **Note**: For help understanding the data drift metrics, see the [How to monitor datasets](https://docs.microsoft.com/azure/machine-learning/how-to-monitor-datasets#understanding-data-drift-results) in the Azure Machine Learning documentation.\n",
    "\n",
    "## Explore further\n",
    "\n",
    "This lab is designed to introduce you to the concepts and principles of data drift monitoring. To learn more about monitoring data drift using datasets, see the [Detect data drift on datasets](https://docs.microsoft.com/azure/machine-learning/how-to-monitor-datasets) in the Azure machine Learning documentation.\n",
    "\n",
    "You can also collect data from published services and use it as a target dataset for datadrift monitoring. See [Collect data from models in production](https://docs.microsoft.com/azure/machine-learning/how-to-enable-data-collection) for details.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
